{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines and Orchestration with Prefect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è  **Version**: This module has been created using Prefect 2.13.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Useful functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 - From previous lessons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib/config.py\n",
    "CATEGORICAL_COLS = [\"PULocationID\", \"DOLocationID\", \"passenger_count\"]\n",
    "\n",
    "DATA_DIRPATH = \"../../data\"\n",
    "MODELS_DIRPATH = \"../../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib/preprocessing.py\n",
    "from typing import List, Tuple\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from loguru import logger\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from prefect import flow, task, serve, aserve\n",
    "\n",
    "def compute_target(\n",
    "    df: pd.DataFrame, pickup_column: str = \"tpep_pickup_datetime\", dropoff_column: str = \"tpep_dropoff_datetime\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute the trip duration in minutes based on pickup and dropoff time\"\"\"\n",
    "    df[\"duration\"] = df[dropoff_column] - df[pickup_column]\n",
    "    df[\"duration\"] = df[\"duration\"].dt.total_seconds() / 60\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_outliers(df: pd.DataFrame, min_duration: int = 1, max_duration: int = 60) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows corresponding to negative/zero\n",
    "    and too high target' values from the dataset\n",
    "    \"\"\"\n",
    "    return df[df[\"duration\"].between(min_duration, max_duration)]\n",
    "\n",
    "\n",
    "def encode_categorical_cols(df: pd.DataFrame, categorical_cols: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Encode categorical columns as strings\"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = CATEGORICAL_COLS\n",
    "    df.loc[:, categorical_cols] = df[categorical_cols].fillna(-1).astype(\"int\")\n",
    "    df.loc[:, categorical_cols] = df[categorical_cols].astype(\"str\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_x_y(\n",
    "    df: pd.DataFrame,\n",
    "    categorical_cols: List[str] = None,\n",
    "    dv: DictVectorizer = None,\n",
    "    with_target: bool = True,\n",
    ") -> Tuple[scipy.sparse.csr_matrix, np.ndarray, DictVectorizer]:\n",
    "    \"\"\"Extract X and y from the dataframe\"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = CATEGORICAL_COLS\n",
    "    dicts = df[categorical_cols].to_dict(orient=\"records\")\n",
    "\n",
    "    y = None\n",
    "    if with_target:\n",
    "        if dv is None:\n",
    "            dv = DictVectorizer()\n",
    "            dv.fit(dicts)\n",
    "        y = df[\"duration\"].values\n",
    "\n",
    "    x = dv.transform(dicts)\n",
    "    return x, y, dv\n",
    "\n",
    "\n",
    "def process_data(filepath: str, dv=None, with_target: bool = True) -> scipy.sparse.csr_matrix:\n",
    "    \"\"\"\n",
    "    Load data from a parquet file\n",
    "    Compute target (duration column) and apply threshold filters (optional)\n",
    "    Turn features to sparce matrix\n",
    "    :return The sparce matrix, the target' values and the\n",
    "    dictvectorizer object if needed.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(filepath)\n",
    "    if with_target:\n",
    "        logger.debug(f\"{filepath} | Computing target...\")\n",
    "        df1 = compute_target(df)\n",
    "        logger.debug(f\"{filepath} | Filtering outliers...\")\n",
    "        df2 = filter_outliers(df1)\n",
    "        logger.debug(f\"{filepath} | Encoding categorical columns...\")\n",
    "        df3 = encode_categorical_cols(df2)\n",
    "        logger.debug(f\"{filepath} | Extracting X and y...\")\n",
    "        return extract_x_y(df3, dv=dv)\n",
    "    else:\n",
    "        logger.debug(f\"{filepath} | Encoding categorical columns...\")\n",
    "        df1 = encode_categorical_cols(df)\n",
    "        logger.debug(f\"{filepath} | Extracting X and y...\")\n",
    "        return extract_x_y(df1, dv=dv, with_target=with_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-2 Helpers for this session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also have other helpers to show you prefect's features in the `helpers.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib/helpers.py\n",
    "from typing import Any\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    return loaded_obj\n",
    "\n",
    "\n",
    "def save_pickle(path: str, obj: Any):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Create workflow functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create five functions to complete the ML process :\n",
    "- `train_model`\n",
    "- `predict`\n",
    "- `evaluate_model`\n",
    "- A workflow function to perform the whole training process `train_model_workflow`\n",
    "    - Process data\n",
    "    - Train model\n",
    "    - Evaluate model\n",
    "- A workflow function to perform the whole prediction process `batch_predict_workflow`\n",
    "    - Process data without target column\n",
    "    - Predict\n",
    "\n",
    "\n",
    "For the last two functions, you can start without saving / loading artifacts add these steps after.\n",
    "Please think about what artifacts you'll need to save and load to pass from training to predict workflows.\n",
    "\n",
    "Start by coding these functions here in the notebook\n",
    "\n",
    "Then, test your code with the downloaded data (e.g. January to train and February to predict).\n",
    "\n",
    "Finally, copy your code in the `lib` folder in the `modeling.py` and `workflows.py` files and test your workflows again using such a command:\n",
    "\n",
    "```bash\n",
    "python lib/workflows.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "@task(name=\"train_model_task\", retries=3, retry_delay_seconds=60)\n",
    "def train_model(X: scipy.sparse.csr_matrix, y: np.ndarray) -> LinearRegression:\n",
    "    \"\"\"Train a linear regression model on the input data.\"\"\"\n",
    "    logger.info(\"Training linear regression model...\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    return lr\n",
    "\n",
    "\n",
    "@task(name=\"predict_task\")\n",
    "def predict(X: scipy.sparse.csr_matrix, model: LinearRegression) -> np.ndarray:\n",
    "    \"\"\"Generate predictions using the trained model.\"\"\"\n",
    "    logger.info(\"Generating predictions...\")\n",
    "    return model.predict(X)\n",
    "\n",
    "\n",
    "@task(name=\"evaluate_model_task\")\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Calculate RMSE between true and predicted values.\"\"\"\n",
    "    logger.info(\"Evaluating model performance...\")\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "@flow(name=\"train_model_workflow\", log_prints=True)\n",
    "def train_model_workflow(\n",
    "    train_filepath: str,\n",
    "    test_filepath: str,\n",
    "    artifacts_filepath: Optional[str] = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Complete training workflow including data processing, model training, and evaluation.\n",
    "    \n",
    "    Args:\n",
    "        train_filepath: Path to training data\n",
    "        test_filepath: Path to test data\n",
    "        artifacts_filepath: Optional path to save model artifacts\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training results and metrics\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting training workflow with data from {train_filepath}\")\n",
    "    \n",
    "    # Process training data\n",
    "    train_data = process_data(train_filepath)\n",
    "    X_train, y_train, dv = train_data\n",
    "    \n",
    "    # Process test data using same DictVectorizer\n",
    "    test_data = process_data(test_filepath, dv)\n",
    "    X_test, y_test, _ = test_data\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Generate predictions\n",
    "    train_pred = predict(X_train, model)\n",
    "    test_pred = predict(X_test, model)\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_rmse = evaluate_model(y_train, train_pred)\n",
    "    test_rmse = evaluate_model(y_test, test_pred)\n",
    "    \n",
    "    logger.info(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "    logger.info(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "    \n",
    "    # Save artifacts if path provided\n",
    "    if artifacts_filepath:\n",
    "        logger.info(f\"Saving model artifacts to {artifacts_filepath}\")\n",
    "        save_pickle(f\"{artifacts_filepath}/model.pkl\", model)\n",
    "        save_pickle(f\"{artifacts_filepath}/dv.pkl\", dv)\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"dv\": dv,\n",
    "        \"metrics\": {\n",
    "            \"train_rmse\": train_rmse,\n",
    "            \"test_rmse\": test_rmse\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "@flow(name=\"batch_predict_workflow\", log_prints=True)\n",
    "def batch_predict_workflow(\n",
    "    input_filepath: str,\n",
    "    model: Optional[LinearRegression] = None,\n",
    "    dv: Optional[DictVectorizer] = None,\n",
    "    artifacts_filepath: Optional[str] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prediction workflow for batch data.\n",
    "    \n",
    "    Args:\n",
    "        input_filepath: Path to input data for predictions\n",
    "        model: Optional pre-loaded model\n",
    "        dv: Optional pre-loaded DictVectorizer\n",
    "        artifacts_filepath: Optional path to load model artifacts\n",
    "    \n",
    "    Returns:\n",
    "        Array of predictions\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting prediction workflow with data from {input_filepath}\")\n",
    "    \n",
    "    # Load artifacts if not provided\n",
    "    if artifacts_filepath and (model is None or dv is None):\n",
    "        logger.info(f\"Loading model artifacts from {artifacts_filepath}\")\n",
    "        model = load_pickle(f\"{artifacts_filepath}/model.pkl\")\n",
    "        dv = load_pickle(f\"{artifacts_filepath}/dv.pkl\")\n",
    "    \n",
    "    # Process input data\n",
    "    X, _, _ = process_data(input_filepath, dv, with_target=False)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = predict(X, model)\n",
    "    \n",
    "    logger.info(f\"Generated {len(predictions)} predictions\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Create Prefect tasks and flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Setup and explore Prefect\n",
    "\n",
    "We are going to use [Prefect](https://docs.prefect.io/2.6/tutorials/first-steps/), an Open Source orchestration tool with a Python SDK.\n",
    "\n",
    "\n",
    "**WINDOWS USERS**:\n",
    "\n",
    "You might run into issues with Prefect on Windows. If you do, please follow [Prefects instructions](https://docs.prefect.io/2.13.7/getting-started/installation/#install-prefect) to install Prefect on your machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 Setup Prefect UI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to implement tasks and flows with prefect, let's set up the UI in order to have a good visualization of our work.\n",
    "\n",
    "Steps :\n",
    "\n",
    "- Set an API URL for your local server to make sure that your workflow will be tracked by this specific instance :\n",
    "```\n",
    "prefect config set PREFECT_API_URL=http://0.0.0.0:4200/api\n",
    "```\n",
    "\n",
    "- Check you have SQLite installed ([Prefect backend database system](https://docs.prefect.io/2.13.7/getting-started/installation/#external-requirements)):\n",
    "```\n",
    "sqlite3 --version \n",
    "```\n",
    "\n",
    "- Start a local prefect server :\n",
    "```\n",
    "prefect server start --host 0.0.0.0\n",
    "```\n",
    "\n",
    "If you want to reset the database, run :\n",
    "```\n",
    "prefect server database reset\n",
    "```\n",
    "\n",
    "\n",
    "You can visit the UI at http://0.0.0.0:4200/dashboard\n",
    "\n",
    "![](images/starting_page.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Prefect tasks and flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prefect uses tasks and flows to build workflows](https://docs.prefect.io/2.13.7/tutorial/flows/).\n",
    "- Flows are like functions. They can take inputs, perform work, and return an output. In fact, you can turn any function into a Prefect flow by adding the @flow decorator\n",
    "- A task is any Python function decorated with a @task decorator called within a flow. You can think of a flow as a recipe for connecting a known sequence of tasks together. Tasks, and the dependencies between them, are displayed in the flow run graph, enabling you to break down a complex flow into something you can observe, understand and control at a more granular level.\n",
    "    - All tasks must be called from within a flow. Tasks may not call other tasks directly.\n",
    "    - Not all functions in a flow need be tasks. Use them only when their features are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "import httpx\n",
    "from prefect import flow, task\n",
    "\n",
    "\n",
    "@task\n",
    "def get_url(url: str, params: dict = None):\n",
    "    response = httpx.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "@flow(retries=3, retry_delay_seconds=5, log_prints=True)\n",
    "def get_repo_info(repo_name: str = \"PrefectHQ/prefect\"):\n",
    "    url = f\"https://api.github.com/repos/{repo_name}\"\n",
    "    repo_stats = get_url(url)\n",
    "    print(f\"{repo_name} repository statistics ü§ì:\")\n",
    "    print(f\"Stars üå† : {repo_stats['stargazers_count']}\")\n",
    "    print(f\"Forks üç¥ : {repo_stats['forks_count']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Create Prefect tasks and flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Create tasks and flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the decorators `@task` and `@flow` to create your first prefect flow : The Processing flow.\n",
    "\n",
    "Prefect will try to use by default different thread to run each task. If you want sequential steps, introduce this dependencies through the name of each task output.\n",
    "\n",
    "\n",
    "Steps:\n",
    "- Create a task for each function you created in the previous section. You can start by doing these in the notebook.\n",
    "- Test your code by calling the flows run with downloaded data (this can be done in the notebook too).\n",
    "- Update your files in the `lib` folder. You should now have completed all files except `deployment.py`.\n",
    "\n",
    "\n",
    "You can see registered flows in the UI :\n",
    "![Flows in Prefect UI](images/flows_ui.png)\n",
    "\n",
    "\n",
    "And visualize the run of a flow :\n",
    "![Flows in Prefect UI](images/flow_run.png)\n",
    "\n",
    "\n",
    "> [!Warning]\n",
    "> **Typing tasks and flows in prefect** :\n",
    "> Typing tasks in prefect is done as with any python code.\n",
    "> For flows, either use `validate_parameters=False` or define pydantic models for prefect to understand your NON DEFAULT typing (see extra section).\n",
    "> But if all tasks are typed, since flows are just set of tasks, it should be all good if we don't want to add a layer of complexity\n",
    "> `Default types` : str, int ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 Customize your flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can configure the properties and special behavior for your prefect tasks/flow in the decorator.\n",
    "For example, you can tell if you want to retry on a failure, set name or tags, etc... \\\n",
    "An example is given in the `helpers.py` file.\n",
    "```\n",
    "@task('name=failure_task', tags=['fails'], retries=3, retry_delay_seconds=60)\n",
    "def func():\n",
    "  ...\n",
    "\n",
    "```\n",
    "\n",
    "- Add names, tasks, and desired behavior to your tasks/flows\n",
    "- Test your code\n",
    "- Visualize in the local prefect UI\n",
    "\n",
    "If a task fails in the flow, it is possible to visualize which task fail and access the full log and traceback error\n",
    "by clicking on the tasks. \\\n",
    "We can also access run information inside de `state` object that can be returned by the flows using python code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Deploy your flows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the workflows are defined, we can now schedule automatics runs for these pipelines.\n",
    "Let's assume that we have a process that tells us that our model need to be retrained weekly based on some performance analysis. We also receive data to predict each hour.\n",
    "\n",
    "Use prefect deployment object in order to :\n",
    "- Schedule complete ml process to run weekly\n",
    "- Schedule prediction pipeline to run each hour\n",
    "\n",
    "\n",
    "**Please note that you can test your code with the `to_deployment` here, however you'll have to move to scripts to test the deployment with `serve`.**\n",
    "\n",
    "You can deploy your flows by following [Prefect documentation here](https://docs.prefect.io/2.13.7/tutorial/deployments/#running-multiple-deployments-at-once).\n",
    "\n",
    "‚ö†Ô∏è  Serving a model with prefect is a long-running command, meaning that you will need to run it in a separate terminal or in the background.\n",
    "Interupting the command will stop the deployment, but you'll be still be able to see it the UI.\n",
    "\n",
    "In the UI, you should be able see deployments:\n",
    "![Deployments in Prefect UI](images/deployments.png)\n",
    "\n",
    "\n",
    "And the scheduled runs for one deployment:\n",
    "![Scheduled runs in Prefect UI](images/scheduled_runs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Your flow </span><span style=\"color: #008000; text-decoration-color: #008000\">'hello-world'</span><span style=\"color: #008000; text-decoration-color: #008000\"> is being served and polling for scheduled runs!</span>\n",
       "\n",
       "To trigger a run for this flow, use the following command:\n",
       "\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">        $ prefect deployment run </span><span style=\"color: #000080; text-decoration-color: #000080\">'hello-world/my-first-deployment'</span>\n",
       "\n",
       "You can also run your flow via the Prefect UI: <span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">http://127.0.0.1:4200/deployments/deployment/</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;coroutine</span><span style=\"color: #000080; text-decoration-color: #000080\"> object sync_compatible.&lt;locals&gt;.coroutine_wrapper.&lt;locals&gt;.ctx_call at </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0x000002534A3740B0</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&gt;</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mYour flow \u001b[0m\u001b[32m'hello-world'\u001b[0m\u001b[32m is being served and polling for scheduled runs!\u001b[0m\n",
       "\n",
       "To trigger a run for this flow, use the following command:\n",
       "\n",
       "\u001b[34m        \u001b[0m\u001b[34m$ prefect deployment run \u001b[0m\u001b[34m'hello-world/my-first-deployment'\u001b[0m\n",
       "\n",
       "You can also run your flow via the Prefect UI: \u001b[4;34mhttp://127.0.0.1:4200/deployments/deployment/\u001b[0m\u001b[1;34m<\u001b[0m\u001b[1;34mcoroutine\u001b[0m\u001b[34m object sync_compatible.<locals>.coroutine_wrapper.<locals>.ctx_call at \u001b[0m\u001b[1;34m0x000002534A3740B0\u001b[0m\u001b[1;34m>\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# creates a deployment and stays running to monitor for work instructions \u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# generated on the server\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mhello_world\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy-first-deployment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monboarding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoodbye\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# hello_world_deployment = hello_world.to_deployment(\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#     name='Hello world Deployment',\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#     version='0.1.0',\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# # Serve the deployments\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# serve(training_deployment, prediction_deployment)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vinay\\anaconda3\\envs\\mlops-course\\lib\\site-packages\\prefect\\flows.py:917\u001b[0m, in \u001b[0;36mFlow.serve\u001b[1;34m(self, name, interval, cron, rrule, paused, schedules, global_limit, triggers, parameters, description, tags, version, enforce_parameter_schema, pause_on_shutdown, print_starting_message, limit, webserver, entrypoint_type)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwebserver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwebserver\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m         asyncio\u001b[38;5;241m.\u001b[39mrun(runner\u001b[38;5;241m.\u001b[39mstart(webserver\u001b[38;5;241m=\u001b[39mwebserver))\n",
      "File \u001b[1;32mc:\\Users\\vinay\\anaconda3\\envs\\mlops-course\\lib\\asyncio\\base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \n\u001b[0;32m    616\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m--> 625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[0;32m    628\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vinay\\anaconda3\\envs\\mlops-course\\lib\\asyncio\\base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    587\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "# hello_world.py\n",
    "from prefect import flow, serve\n",
    "\n",
    "\n",
    "@flow(name=\"Hello world\")\n",
    "def hello_world(name: str = \"world\"):\n",
    "    print(f\"Hello {name}!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # hello_world_deployment = hello_world.to_deployment(\n",
    "    #     name='Hello world Deployment',\n",
    "    #     version='0.1.0',\n",
    "    #     tags=['hello world'],\n",
    "    #     interval=600\n",
    "    #     parameters={\n",
    "    #         'name': 'John Doe'\n",
    "    #     }\n",
    "    # )\n",
    "    # # Above: can be tested in notebook. Below: must be called from python script\n",
    "    # serve(hello_world_deployment)\n",
    "    # Example deployment configuration\n",
    "    training_deployment = train_model_workflow.to_deployment(\n",
    "        name=\"Taxi Duration Training\",\n",
    "        version=\"1.0.0\",\n",
    "        tags=[\"ml\", \"training\"],\n",
    "        interval=604800  # Weekly training\n",
    "    )\n",
    "    \n",
    "    prediction_deployment = batch_predict_workflow.to_deployment(\n",
    "        name=\"Taxi Duration Prediction\",\n",
    "        version=\"1.0.0\",\n",
    "        tags=[\"ml\", \"prediction\"],\n",
    "        interval=3600  # Hourly predictions\n",
    "    )\n",
    "    \n",
    "    # Serve the deployments\n",
    "    serve(training_deployment, prediction_deployment)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Extra concepts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 Prefect workers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 Prefect typing using Pydantic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
